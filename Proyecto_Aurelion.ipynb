{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56151ae",
   "metadata": {},
   "source": [
    "## üì• 1. Importaci√≥n y Carga de los 4 Dataframes Limpios\n",
    "\n",
    "Cargaremos los 4 dataframes desde los notebooks ya ejecutados. Esto requiere que los notebooks anteriores hayan sido ejecutados y los dataframes est√©n disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c8fb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerias cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importaciones necesarias\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Opciones de pandas para debugging\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print('Librerias cargadas correctamente.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "auto_carga_datos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK ] Clientes                 -> (100, 5) | fuente: processed\\clientes_limpio.csv\n",
      "[OK ] Ventas (cabecera)        -> (120, 12) | fuente: processed\\ventas_limpio.csv\n",
      "[OK ] Detalle de ventas        -> (343, 6) | fuente: processed\\detalle_ventas_limpio.csv\n",
      "[OK ] Productos                -> (100, 4) | fuente: processed\\productos_limpio.csv\n",
      "Dataframes limpios disponibles en memoria.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuracion de rutas y artefactos procesados (ETL)\n",
    "PROJECT_DIR = Path.cwd()\n",
    "PATH_PROCESSED = PROJECT_DIR / 'processed'\n",
    "PATH_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_FINAL_PATH = PROJECT_DIR / 'db' / 'final' / 'Base_Final_Aurelion.csv'\n",
    "\n",
    "DATA_SPECS = {\n",
    "    'df_clientes_True': {\n",
    "        'label': 'Clientes',\n",
    "        'filename': 'clientes_limpio.csv',\n",
    "        'parse_dates': ['fecha_alta'],\n",
    "    },\n",
    "    'df_ventas_True': {\n",
    "        'label': 'Ventas (cabecera)',\n",
    "        'filename': 'ventas_limpio.csv',\n",
    "        'parse_dates': ['fecha'],\n",
    "    },\n",
    "    'df_detalle_ventas_True': {\n",
    "        'label': 'Detalle de ventas',\n",
    "        'filename': 'detalle_ventas_limpio.csv',\n",
    "        'parse_dates': [],\n",
    "    },\n",
    "    'df_productos_True': {\n",
    "        'label': 'Productos',\n",
    "        'filename': 'productos_limpio.csv',\n",
    "        'parse_dates': [],\n",
    "    },\n",
    "}\n",
    "\n",
    "def load_clean_dataframe(var_name, spec):\n",
    "    file_path = PATH_PROCESSED / spec['filename']\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"No se encontro el archivo esperado: {file_path}\")\n",
    "    df = pd.read_csv(file_path, parse_dates=spec.get('parse_dates') or [])\n",
    "    print(f\"[OK ] {spec['label']:<24} -> {df.shape} | fuente: {file_path.relative_to(PROJECT_DIR)}\")\n",
    "    return df\n",
    "\n",
    "loaded_frames = {var: load_clean_dataframe(var, spec) for var, spec in DATA_SPECS.items()}\n",
    "globals().update(loaded_frames)\n",
    "print(\"Dataframes limpios disponibles en memoria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe5d5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK  Clientes                       -> (100, 5) | archivo: OK\n",
      "OK  Ventas (cabecera)              -> (120, 12) | archivo: OK\n",
      "OK  Detalle de ventas              -> (343, 6) | archivo: OK\n",
      "OK  Productos                      -> (100, 4) | archivo: OK\n",
      "Todos los dataframes limpios est√°n en memoria.\n",
      "1. df_clientes_True -> Clientes\n",
      "--------------------------------------------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [id_cliente, nombre_cliente, email, ciudad, fecha_alta]\n",
      "Index: []\n",
      "Columnas: ['id_cliente', 'nombre_cliente', 'email', 'ciudad', 'fecha_alta']\n",
      "Registros: 100\n",
      "2. df_ventas_True -> Ventas (cabecera)\n",
      "--------------------------------------------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [id_venta, fecha, id_cliente, medio_pago_efectivo, medio_pago_qr, medio_pago_tarjeta, medio_pago_transferencia, anio, mes, dia_semana, trimestre, transacciones_cliente]\n",
      "Index: []\n",
      "Columnas: ['id_venta', 'fecha', 'id_cliente', 'medio_pago_efectivo', 'medio_pago_qr', 'medio_pago_tarjeta', 'medio_pago_transferencia', 'anio', 'mes', 'dia_semana', 'trimestre', 'transacciones_cliente']\n",
      "Registros: 120\n",
      "3. df_detalle_ventas_True -> Detalle de ventas\n",
      "--------------------------------------------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [id_venta, id_producto, cantidad, precio_unitario, importe, importe_std]\n",
      "Index: []\n",
      "Columnas: ['id_venta', 'id_producto', 'cantidad', 'precio_unitario', 'importe', 'importe_std']\n",
      "Registros: 343\n",
      "4. df_productos_True -> Productos\n",
      "--------------------------------------------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [id_producto, nombre_producto, categoria, precio_unitario]\n",
      "Index: []\n",
      "Columnas: ['id_producto', 'nombre_producto', 'categoria', 'precio_unitario']\n",
      "Registros: 100\n"
     ]
    }
   ],
   "source": [
    "# Funcion para verificar y reportar disponibilidad de dataframes\n",
    "\n",
    "def check_dataframes():\n",
    "    missing = []\n",
    "    for var_name, spec in DATA_SPECS.items():\n",
    "        processed_path = PATH_PROCESSED / spec['filename']\n",
    "        artifact_status = 'OK' if processed_path.exists() else 'PENDIENTE'\n",
    "        if var_name in globals():\n",
    "            df_temp = globals()[var_name]\n",
    "            print(f\"OK  {spec['label']:30s} -> {df_temp.shape} | archivo: {artifact_status}\")\n",
    "        else:\n",
    "            missing.append(spec['label'])\n",
    "            print(f\"X   {spec['label']:30s} -> NO EN MEMORIA | archivo: {artifact_status}\")\n",
    "    return missing\n",
    "\n",
    "missing_specs = check_dataframes()\n",
    "all_available = len(missing_specs) == 0\n",
    "\n",
    "if all_available:\n",
    "    print(\"Todos los dataframes limpios est√°n en memoria.\")\n",
    "    for idx, (var_name, spec) in enumerate(DATA_SPECS.items(), start=1):\n",
    "        df_temp = globals()[var_name]\n",
    "        print(f\"{idx}. {var_name} -> {spec['label']}\")\n",
    "        print('-'*80)\n",
    "        print(df_temp.head(0))\n",
    "        print(f\"Columnas: {list(df_temp.columns)}\")\n",
    "        print(f\"Registros: {len(df_temp)}\")\n",
    "else:\n",
    "    print(f\" No es posible avanzar. Dataframes faltantes: {missing_specs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afbcb4",
   "metadata": {},
   "source": [
    "## üîó 3. Preparaci√≥n: Renombrado de Claves For√°neas\n",
    "\n",
    "En la tabla de detalle de ventas, si existe un nombre diferente para la FK de producto (ej: FK_producto), lo renombramos a `id_producto` para que el merge sea un√≠voco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc66d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üîß PREPARACI√ìN: Validaci√≥n y renombrado de columnas\n",
      "====================================================================================================\n",
      "\n",
      "‚úì Columna id_producto ya existe en df_detalle_ventas_True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üìå Validaci√≥n de Integridad Referencial:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì FK id_cliente en ventas:\n",
      "OK (67 referencias v√°lidas)\n",
      "‚úì FK id_venta en detalle:\n",
      "OK (120 referencias v√°lidas)\n",
      "‚úì FK id_producto en detalle:\n",
      "OK (95 referencias v√°lidas)\n"
     ]
    }
   ],
   "source": [
    "if all_available:\n",
    "    print('='*100)\n",
    "    print('üîß PREPARACI√ìN: Validaci√≥n y renombrado de columnas')\n",
    "    print('='*100)\n",
    "    \n",
    "    # Verificar y renombrar columna de producto en detalle_ventas si es necesario\n",
    "    if 'FK_producto' in df_detalle_ventas_True.columns:\n",
    "        print('\\n‚úì Renombrando FK_producto ‚Üí id_producto en df_detalle_ventas_True')\n",
    "        df_detalle_ventas_True.rename(columns={'FK_producto': 'id_producto'}, inplace=True)\n",
    "    elif 'id_producto' not in df_detalle_ventas_True.columns:\n",
    "        print('\\n‚ö†Ô∏è  Aviso: No se encontr√≥ FK_producto ni id_producto en detalle_ventas')\n",
    "        print(f'   Columnas disponibles: {list(df_detalle_ventas_True.columns)}')\n",
    "    else:\n",
    "        print('\\n‚úì Columna id_producto ya existe en df_detalle_ventas_True')\n",
    "    \n",
    "    # Validar que todos los FK referencias existan en las PK correspondientes\n",
    "    print('\\n' + '-'*80)\n",
    "    print('üìå Validaci√≥n de Integridad Referencial:')\n",
    "    print('-'*80)\n",
    "    \n",
    "    # FK id_cliente en ventas\n",
    "    clientes_en_ventas = set(df_ventas_True['id_cliente'].dropna().unique())\n",
    "    clientes_existentes = set(df_clientes_True['id_cliente'].unique())\n",
    "    clientes_huerfanos = clientes_en_ventas - clientes_existentes\n",
    "    print(f'\\n‚úì FK id_cliente en ventas:',)\n",
    "    if not clientes_huerfanos:\n",
    "        print(f'OK ({len(clientes_en_ventas)} referencias v√°lidas)')\n",
    "    else:\n",
    "        print(f'INCONSISTENCIA: {len(clientes_huerfanos)} hu√©rfanos')\n",
    "    \n",
    "    # FK id_venta en detalle\n",
    "    ventas_en_detalle = set(df_detalle_ventas_True['id_venta'].dropna().unique())\n",
    "    ventas_existentes = set(df_ventas_True['id_venta'].unique())\n",
    "    ventas_huerfanas = ventas_en_detalle - ventas_existentes\n",
    "    print(f'‚úì FK id_venta en detalle:',)\n",
    "    if not ventas_huerfanas:\n",
    "        print(f'OK ({len(ventas_en_detalle)} referencias v√°lidas)')\n",
    "    else:\n",
    "        print(f'INCONSISTENCIA: {len(ventas_huerfanas)} hu√©rfanos')\n",
    "    \n",
    "    # FK id_producto en detalle\n",
    "    productos_en_detalle = set(df_detalle_ventas_True['id_producto'].dropna().unique())\n",
    "    productos_existentes = set(df_productos_True['id_producto'].unique())\n",
    "    productos_huerfanos = productos_en_detalle - productos_existentes\n",
    "    print(f'‚úì FK id_producto en detalle:',)\n",
    "    if not productos_huerfanos:\n",
    "        print(f'OK ({len(productos_en_detalle)} referencias v√°lidas)')\n",
    "    else:\n",
    "        print(f'INCONSISTENCIA: {len(productos_huerfanos)} hu√©rfanos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc2bf7",
   "metadata": {},
   "source": [
    "## üîÄ 4. Merge de Tablas - Consolidaci√≥n de Base Final\n",
    "\n",
    "Realizamos los merges en orden siguiendo la estructura de relaciones:\n",
    "1. `df_clientes_True` ‚Üê merge ‚Üê `df_ventas_True` (on=id_cliente)\n",
    "2. Resultado ‚Üê merge ‚Üê `df_detalle_ventas_True` (on=id_venta)\n",
    "3. Resultado ‚Üê merge ‚Üê `df_productos_True` (on=id_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b8e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " MERGE - Consolidacion de Base Final\n",
      "====================================================================================================\n",
      "Columnas agregadas desde productos: ['id_producto', 'nombre_producto', 'categoria']\n",
      "Dimensiones detalle+productos: (343, 8)\n",
      "Columnas agregadas desde clientes: ['id_cliente', 'nombre_cliente', 'ciudad']\n",
      "Dimensiones ventas+clientes: (120, 14)\n",
      "Dimensiones finales tras merge: (343, 21)\n",
      "Columnas duplicadas detectadas: []\n",
      "\n",
      "Merge completado exitosamente!\n",
      "\n",
      " MERGE - Consolidacion de Base Final\n",
      "====================================================================================================\n",
      "Columnas agregadas desde productos: ['id_producto', 'nombre_producto', 'categoria']\n",
      "Dimensiones detalle+productos: (343, 8)\n",
      "Columnas agregadas desde clientes: ['id_cliente', 'nombre_cliente', 'ciudad']\n",
      "Dimensiones ventas+clientes: (120, 14)\n",
      "Dimensiones finales tras merge: (343, 21)\n",
      "Columnas duplicadas detectadas: []\n",
      "\n",
      "Merge completado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if all_available:\n",
    "    print('='*100)\n",
    "    print(' MERGE - Consolidacion de Base Final')\n",
    "    print('='*100)\n",
    "\n",
    "    detalle_cols = list(df_detalle_ventas_True.columns)\n",
    "    prefer_prod = ['id_producto', 'nombre_producto', 'categoria', 'marca', 'subcategoria', 'tipo_producto']\n",
    "    cols_prod = []\n",
    "    for col in prefer_prod:\n",
    "        if col == 'id_producto' and col in df_productos_True.columns:\n",
    "            if col not in cols_prod:\n",
    "                cols_prod.append(col)\n",
    "        elif col in df_productos_True.columns and col not in detalle_cols:\n",
    "            cols_prod.append(col)\n",
    "    if 'id_producto' not in cols_prod and 'id_producto' in df_productos_True.columns:\n",
    "        cols_prod.insert(0, 'id_producto')\n",
    "    if len(cols_prod) == 1:\n",
    "        extras = [c for c in df_productos_True.columns if c not in detalle_cols and c != 'id_producto']\n",
    "        cols_prod.extend(extras)\n",
    "    df_productos_dim = df_productos_True[cols_prod].copy()\n",
    "    print(f\"Columnas agregadas desde productos: {cols_prod}\")\n",
    "\n",
    "    df_detalle_prod = df_detalle_ventas_True.merge(\n",
    "        df_productos_dim,\n",
    "        on='id_producto',\n",
    "        how='left',\n",
    "        validate='m:1'\n",
    "    )\n",
    "    print(f\"Dimensiones detalle+productos: {df_detalle_prod.shape}\")\n",
    "\n",
    "    ventas_cols = list(df_ventas_True.columns)\n",
    "    prefer_cli = ['id_cliente', 'nombre_cliente', 'segmento', 'provincia', 'ciudad', 'region', 'pais', 'categoria_cliente']\n",
    "    cols_cli = []\n",
    "    for col in prefer_cli:\n",
    "        if col == 'id_cliente' and col in df_clientes_True.columns:\n",
    "            if col not in cols_cli:\n",
    "                cols_cli.append(col)\n",
    "        elif col in df_clientes_True.columns and col not in ventas_cols:\n",
    "            cols_cli.append(col)\n",
    "    if 'id_cliente' not in cols_cli and 'id_cliente' in df_clientes_True.columns:\n",
    "        cols_cli.insert(0, 'id_cliente')\n",
    "    if len(cols_cli) == 1:\n",
    "        extras_cli = [c for c in df_clientes_True.columns if c not in ventas_cols and c != 'id_cliente']\n",
    "        cols_cli.extend(extras_cli)\n",
    "    df_clientes_dim = df_clientes_True[cols_cli].copy()\n",
    "    print(f\"Columnas agregadas desde clientes: {cols_cli}\")\n",
    "\n",
    "    df_ventas_cli = df_ventas_True.merge(\n",
    "        df_clientes_dim,\n",
    "        on='id_cliente',\n",
    "        how='left',\n",
    "        validate='m:1'\n",
    "    )\n",
    "    print(f\"Dimensiones ventas+clientes: {df_ventas_cli.shape}\")\n",
    "\n",
    "    df_final = df_detalle_prod.merge(\n",
    "        df_ventas_cli,\n",
    "        on='id_venta',\n",
    "        how='left',\n",
    "        validate='m:1',\n",
    "        suffixes=('', '_ventas')\n",
    "    )\n",
    "    print(f\"Dimensiones finales tras merge: {df_final.shape}\")\n",
    "\n",
    "    duplicated_cols = df_final.columns[df_final.columns.duplicated()]\n",
    "    print('Columnas duplicadas detectadas:', duplicated_cols.tolist())\n",
    "    if duplicated_cols.any():\n",
    "        df_final = df_final.loc[:, ~df_final.columns.duplicated()]\n",
    "        print('Columnas duplicadas eliminadas tras validar equivalencia.')\n",
    "\n",
    "    base_consolidada = df_final.copy()\n",
    "    print('\\nMerge completado exitosamente!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de60ff8",
   "metadata": {},
   "source": [
    "## üìä 5. Inspecci√≥n de la Base Consolidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18c57271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìã INSPECCI√ìN - Base Consolidada\n",
      "====================================================================================================\n",
      "\n",
      "Primeras 5 filas:\n",
      "--------------------------------------------------------------------------------\n",
      "   id_venta  id_producto  cantidad  precio_unitario  importe  importe_std        nombre_producto  categoria  \\\n",
      "0         1           90         1           2902.0   2902.0    -0.918259    Toallas H√∫medas X50  alimentos   \n",
      "1         2           82         5           2394.0  11970.0     0.806396  Aceitunas Negras 200G  alimentos   \n",
      "2         2           39         5            469.0   2345.0    -1.024196     Helado Vainilla 1L  alimentos   \n",
      "3         2           70         2           4061.0   8122.0     0.074540           Fernet 750Ml  alimentos   \n",
      "4         2           22         1           2069.0   2069.0    -1.076688  Medialunas De Manteca  alimentos   \n",
      "\n",
      "       fecha  id_cliente  medio_pago_efectivo  medio_pago_qr  medio_pago_tarjeta  medio_pago_transferencia  anio  mes  \\\n",
      "0 2024-06-19          62                  0.0            0.0                 1.0                       0.0  2024    6   \n",
      "1 2024-03-17          49                  0.0            1.0                 0.0                       0.0  2024    3   \n",
      "2 2024-03-17          49                  0.0            1.0                 0.0                       0.0  2024    3   \n",
      "3 2024-03-17          49                  0.0            1.0                 0.0                       0.0  2024    3   \n",
      "4 2024-03-17          49                  0.0            1.0                 0.0                       0.0  2024    3   \n",
      "\n",
      "  dia_semana  trimestre  transacciones_cliente    nombre_cliente      ciudad  \n",
      "0  Mi√©rcoles          2                      2  Guadalupe Romero  carlos paz  \n",
      "1    Domingo          1                      4      Olivia Gomez  rio cuarto  \n",
      "2    Domingo          1                      4      Olivia Gomez  rio cuarto  \n",
      "3    Domingo          1                      4      Olivia Gomez  rio cuarto  \n",
      "4    Domingo          1                      4      Olivia Gomez  rio cuarto  \n",
      "\n",
      "\n",
      "√öltimas 5 filas:\n",
      "--------------------------------------------------------------------------------\n",
      "     id_venta  id_producto  cantidad  precio_unitario  importe  importe_std        nombre_producto  categoria  \\\n",
      "338       118           70         2           4061.0   8122.0     0.074540           Fernet 750Ml  alimentos   \n",
      "339       118           93         3           2142.0   6426.0    -0.248025     Cepillo De Dientes   limpieza   \n",
      "340       118           50         2            727.0   1454.0    -1.193656             Az√∫car 1Kg  alimentos   \n",
      "341       119           45         5            745.0   3725.0    -0.761731  Fideos Spaghetti 500G  alimentos   \n",
      "342       120           20         5           1571.0   7855.0     0.023759      Pan Lactal Blanco  alimentos   \n",
      "\n",
      "         fecha  id_cliente  medio_pago_efectivo  medio_pago_qr  medio_pago_tarjeta  medio_pago_transferencia  anio  \\\n",
      "338 2024-02-09          84                  1.0            0.0                 0.0                       0.0  2024   \n",
      "339 2024-02-09          84                  1.0            0.0                 0.0                       0.0  2024   \n",
      "340 2024-02-09          84                  1.0            0.0                 0.0                       0.0  2024   \n",
      "341 2024-02-07          51                  0.0            1.0                 0.0                       0.0  2024   \n",
      "342 2024-04-21          72                  0.0            0.0                 1.0                       0.0  2024   \n",
      "\n",
      "     mes dia_semana  trimestre  transacciones_cliente    nombre_cliente      ciudad  \n",
      "338    2    Viernes          1                      2     Pablo Sanchez     cordoba  \n",
      "339    2    Viernes          1                      2     Pablo Sanchez     cordoba  \n",
      "340    2    Viernes          1                      2     Pablo Sanchez     cordoba  \n",
      "341    2  Mi√©rcoles          1                      2    Agustina Gomez  rio cuarto  \n",
      "342    4    Domingo          2                      4  Camila Rodriguez     cordoba  \n",
      "\n",
      "\n",
      "Tipos de datos:\n",
      "--------------------------------------------------------------------------------\n",
      "id_venta                             int64\n",
      "id_producto                          int64\n",
      "cantidad                             int64\n",
      "precio_unitario                    float64\n",
      "importe                            float64\n",
      "importe_std                        float64\n",
      "nombre_producto                     object\n",
      "categoria                           object\n",
      "fecha                       datetime64[ns]\n",
      "id_cliente                           int64\n",
      "medio_pago_efectivo                float64\n",
      "medio_pago_qr                      float64\n",
      "medio_pago_tarjeta                 float64\n",
      "medio_pago_transferencia           float64\n",
      "anio                                 int64\n",
      "mes                                  int64\n",
      "dia_semana                          object\n",
      "trimestre                            int64\n",
      "transacciones_cliente                int64\n",
      "nombre_cliente                      object\n",
      "ciudad                              object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Valores nulos por columna:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì NO hay valores nulos en la base consolidada\n",
      "\n",
      "\n",
      "Estad√≠sticas generales:\n",
      "--------------------------------------------------------------------------------\n",
      "Dimensiones: (343, 21)\n",
      "Memoria usada: 0.14 MB\n",
      "Clientes √∫nicos: 67\n",
      "Ventas (id_venta) √∫nicas: 120\n",
      "Detalles de venta √∫nicos: N/A\n",
      "Productos √∫nicos: 95\n"
     ]
    }
   ],
   "source": [
    "if 'base_consolidada' in locals():\n",
    "    print('='*100)\n",
    "    print('üìã INSPECCI√ìN - Base Consolidada')\n",
    "    print('='*100)\n",
    "    \n",
    "    print('\\nPrimeras 5 filas:')\n",
    "    print('-'*80)\n",
    "    print(base_consolidada.head())\n",
    "    \n",
    "    print('\\n\\n√öltimas 5 filas:')\n",
    "    print('-'*80)\n",
    "    print(base_consolidada.tail())\n",
    "    \n",
    "    print('\\n\\nTipos de datos:')\n",
    "    print('-'*80)\n",
    "    print(base_consolidada.dtypes)\n",
    "    \n",
    "    print('\\n\\nValores nulos por columna:')\n",
    "    print('-'*80)\n",
    "    nulls = base_consolidada.isnull().sum()\n",
    "    if nulls.sum() == 0:\n",
    "        print('‚úì NO hay valores nulos en la base consolidada')\n",
    "    else:\n",
    "        print(nulls[nulls > 0])\n",
    "    \n",
    "    print('\\n\\nEstad√≠sticas generales:')\n",
    "    print('-'*80)\n",
    "    print(f'Dimensiones: {base_consolidada.shape}')\n",
    "    print(f'Memoria usada: {base_consolidada.memory_usage(deep=True).sum() / (1024**2):.2f} MB')\n",
    "    print(f'Clientes √∫nicos: {base_consolidada[\"id_cliente\"].nunique()}')\n",
    "    print(f'Ventas (id_venta) √∫nicas: {base_consolidada[\"id_venta\"].nunique()}')\n",
    "    print(f'Detalles de venta √∫nicos: {base_consolidada[\"id_detalle\"].nunique() if \"id_detalle\" in base_consolidada.columns else \"N/A\"}')\n",
    "    print(f'Productos √∫nicos: {base_consolidada[\"id_producto\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b37e7",
   "metadata": {},
   "source": [
    "## üßπ 6. Preprocesamiento y Limpieza Adicional\n",
    "\n",
    "En esta secci√≥n aplicamos transformaciones finales para garantizar que la base est√© lista para an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44bdcdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üßπ PREPROCESAMIENTO - Limpieza y Transformaciones Finales\n",
      "====================================================================================================\n",
      "\n",
      "1Ô∏è‚É£  Detecci√≥n de duplicados\n",
      "--------------------------------------------------------------------------------\n",
      "Filas duplicadas: 0\n",
      "\n",
      "2Ô∏è‚É£  Estandarizaci√≥n de nombres de columnas\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì No hay columnas con sufijos _x o _y\n",
      "\n",
      "2b  Columnas duplicadas tras el merge\n",
      "--------------------------------------------------------------------------------\n",
      " No se detectaron columnas duplicadas\n",
      "\n",
      "3Ô∏è‚É£  Reorganizaci√≥n de columnas (orden l√≥gico)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì Columnas reordenadas: 21 columnas\n",
      "  - IDs (PK/FK): 4\n",
      "  - Fechas: 1\n",
      "  - Dinero (precios/importes): 3\n",
      "  - Otros: 13\n",
      "\n",
      "4Ô∏è‚É£  Validaci√≥n de tipos de datos\n",
      "--------------------------------------------------------------------------------\n",
      "id_venta                             int64\n",
      "id_producto                          int64\n",
      "cantidad                             int64\n",
      "id_cliente                           int64\n",
      "fecha                       datetime64[ns]\n",
      "nombre_producto                     object\n",
      "categoria                           object\n",
      "medio_pago_efectivo                float64\n",
      "medio_pago_qr                      float64\n",
      "medio_pago_tarjeta                 float64\n",
      "medio_pago_transferencia           float64\n",
      "anio                                 int64\n",
      "mes                                  int64\n",
      "dia_semana                          object\n",
      "trimestre                            int64\n",
      "transacciones_cliente                int64\n",
      "nombre_cliente                      object\n",
      "ciudad                              object\n",
      "precio_unitario                    float64\n",
      "importe                            float64\n",
      "importe_std                        float64\n",
      "dtype: object\n",
      "\n",
      "‚ú® Preprocesamiento completado\n"
     ]
    }
   ],
   "source": [
    "if 'base_consolidada' in locals():\n",
    "    print('='*100)\n",
    "    print('üßπ PREPROCESAMIENTO - Limpieza y Transformaciones Finales')\n",
    "    print('='*100)\n",
    "    \n",
    "    # Crear copia para no modificar original hasta validar\n",
    "    base_final = base_consolidada.copy()\n",
    "    \n",
    "    # 1. Detectar y manejar duplicados\n",
    "    print('\\n1Ô∏è‚É£  Detecci√≥n de duplicados')\n",
    "    print('-'*80)\n",
    "    dup_count = base_final.duplicated().sum()\n",
    "    print(f'Filas duplicadas: {dup_count}')\n",
    "    if dup_count > 0:\n",
    "        print(f'  (Eliminando {dup_count} filas duplicadas)')\n",
    "        base_final = base_final.drop_duplicates()\n",
    "        print(f'  Nuevo tama√±o: {base_final.shape}')\n",
    "    \n",
    "    # 2. Renombrar columnas para mayor claridad y evitar colisiones\n",
    "    print('\\n2Ô∏è‚É£  Estandarizaci√≥n de nombres de columnas')\n",
    "    print('-'*80)\n",
    "    rename_map = {}\n",
    "    for col in base_final.columns:\n",
    "        # Aqu√≠ puedes definir transformaciones de nombres si es necesario\n",
    "        # Ejemplo: renombrar columnas que tengan sufijos _x o _y\n",
    "        if col.endswith('_x'):\n",
    "            rename_map[col] = col[:-2]\n",
    "        elif col.endswith('_y'):\n",
    "            rename_map[col] = col[:-2]\n",
    "    \n",
    "    if rename_map:\n",
    "        print(f'Columnas renombradas:')\n",
    "        for old, new in rename_map.items():\n",
    "            print(f'  {old:30s} ‚Üí {new}')\n",
    "        base_final.rename(columns=rename_map, inplace=True)\n",
    "    else:\n",
    "        print('‚úì No hay columnas con sufijos _x o _y')\n",
    "    \n",
    "\n",
    "    # 2b. Eliminar columnas duplicadas post-merge\n",
    "    print('\\n2b  Columnas duplicadas tras el merge')\n",
    "    print('-'*80)\n",
    "    duplicated_mask = base_final.columns.duplicated()\n",
    "    if duplicated_mask.any():\n",
    "        dup_names = base_final.columns[duplicated_mask]\n",
    "        dup_list = sorted({str(name) for name in dup_names})\n",
    "        base_final = base_final.loc[:, ~duplicated_mask]\n",
    "        print(f\" Columnas eliminadas: {dup_list}\")\n",
    "    else:\n",
    "        print(' No se detectaron columnas duplicadas')\n",
    "\n",
    "    # 3. Reordenar columnas de forma l√≥gica (primero IDs, luego datos descriptivos)\n",
    "    print('\\n3Ô∏è‚É£  Reorganizaci√≥n de columnas (orden l√≥gico)')\n",
    "    print('-'*80)\n",
    "    id_cols = [c for c in base_final.columns if 'id' in c.lower()]\n",
    "    date_cols = [c for c in base_final.columns if 'fecha' in c.lower() or 'date' in c.lower()]\n",
    "    money_cols = [c for c in base_final.columns if any(x in c.lower() for x in ['precio', 'importe', 'monto', 'total'])]\n",
    "    other_cols = [c for c in base_final.columns if c not in id_cols + date_cols + money_cols]\n",
    "    \n",
    "    new_order = id_cols + date_cols + other_cols + money_cols\n",
    "    base_final = base_final[[c for c in new_order if c in base_final.columns]]\n",
    "    print(f'‚úì Columnas reordenadas: {len(new_order)} columnas')\n",
    "    print(f'  - IDs (PK/FK): {len(id_cols)}')\n",
    "    print(f'  - Fechas: {len(date_cols)}')\n",
    "    print(f'  - Dinero (precios/importes): {len(money_cols)}')\n",
    "    print(f'  - Otros: {len(other_cols)}')\n",
    "    \n",
    "    # 4. Tipos de datos finales\n",
    "    print('\\n4Ô∏è‚É£  Validaci√≥n de tipos de datos')\n",
    "    print('-'*80)\n",
    "    print(base_final.dtypes)\n",
    "    \n",
    "    print('\\n‚ú® Preprocesamiento completado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d19523",
   "metadata": {},
   "source": [
    "## üìÑ 7. Exportaci√≥n a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93309a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " EXPORTACION - Base Final a CSV\n",
      "====================================================================================================\n",
      "\n",
      "--- Info de Base_Final_Aurelion (previa a exportaci√≥n) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343 entries, 0 to 342\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   id_venta                  343 non-null    int64         \n",
      " 1   id_producto               343 non-null    int64         \n",
      " 2   cantidad                  343 non-null    int64         \n",
      " 3   id_cliente                343 non-null    int64         \n",
      " 4   fecha                     343 non-null    datetime64[ns]\n",
      " 5   nombre_producto           343 non-null    object        \n",
      " 6   categoria                 343 non-null    object        \n",
      " 7   medio_pago_efectivo       343 non-null    float64       \n",
      " 8   medio_pago_qr             343 non-null    float64       \n",
      " 9   medio_pago_tarjeta        343 non-null    float64       \n",
      " 10  medio_pago_transferencia  343 non-null    float64       \n",
      " 11  anio                      343 non-null    int64         \n",
      " 12  mes                       343 non-null    int64         \n",
      " 13  dia_semana                343 non-null    object        \n",
      " 14  trimestre                 343 non-null    int64         \n",
      " 15  transacciones_cliente     343 non-null    int64         \n",
      " 16  nombre_cliente            343 non-null    object        \n",
      " 17  ciudad                    343 non-null    object        \n",
      " 18  precio_unitario           343 non-null    float64       \n",
      " 19  importe                   343 non-null    float64       \n",
      " 20  importe_std               343 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(8), object(5)\n",
      "memory usage: 56.4+ KB\n",
      "\n",
      "Guardando Base_Final_Aurelion en: c:\\Users\\Asus\\Desktop\\Augusto Villegas - Proyecto Aurelion\\db\\final\\Base_Final_Aurelion.csv\n",
      "Shape: (343, 21) | Columnas: 21\n",
      "Checksum MD5 (ffe5dbf4668473d1f5250b1541f6bd37) guardado en: c:\\Users\\Asus\\Desktop\\Augusto Villegas - Proyecto Aurelion\\db\\final\\Base_Final_Aurelion.md5\n",
      "\n",
      "--- Estructura de Base_Final_Aurelion (desde CSV) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343 entries, 0 to 342\n",
      "Data columns (total 21 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   id_venta                  343 non-null    int64         \n",
      " 1   id_producto               343 non-null    int64         \n",
      " 2   cantidad                  343 non-null    int64         \n",
      " 3   id_cliente                343 non-null    int64         \n",
      " 4   fecha                     343 non-null    datetime64[ns]\n",
      " 5   nombre_producto           343 non-null    object        \n",
      " 6   categoria                 343 non-null    object        \n",
      " 7   medio_pago_efectivo       343 non-null    float64       \n",
      " 8   medio_pago_qr             343 non-null    float64       \n",
      " 9   medio_pago_tarjeta        343 non-null    float64       \n",
      " 10  medio_pago_transferencia  343 non-null    float64       \n",
      " 11  anio                      343 non-null    int64         \n",
      " 12  mes                       343 non-null    int64         \n",
      " 13  dia_semana                343 non-null    object        \n",
      " 14  trimestre                 343 non-null    int64         \n",
      " 15  transacciones_cliente     343 non-null    int64         \n",
      " 16  nombre_cliente            343 non-null    object        \n",
      " 17  ciudad                    343 non-null    object        \n",
      " 18  precio_unitario           343 non-null    float64       \n",
      " 19  importe                   343 non-null    float64       \n",
      " 20  importe_std               343 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(7), int64(8), object(5)\n",
      "memory usage: 56.4+ KB\n",
      "Columnas duplicadas en Base_Final_Aurelion: []\n",
      "El dataframe en memoria coincide con el CSV exportado.\n",
      "Consolidaci√≥n completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "if 'base_final' in locals():\n",
    "    print('='*100)\n",
    "    print(' EXPORTACION - Base Final a CSV')\n",
    "    print('='*100)\n",
    "\n",
    "    print(\"\\n--- Info de Base_Final_Aurelion (previa a exportaci√≥n) ---\")\n",
    "    # Forzar a que muestre todas las columnas\n",
    "    base_final.info(verbose=True)\n",
    "\n",
    "    BASE_FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\nGuardando Base_Final_Aurelion en: {BASE_FINAL_PATH}\")\n",
    "    base_final.to_csv(BASE_FINAL_PATH, index=False)\n",
    "    print(f\"Shape: {base_final.shape} | Columnas: {len(base_final.columns)}\")\n",
    "\n",
    "    import hashlib\n",
    "\n",
    "    def compute_md5(path_to_file):\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(path_to_file, 'rb') as file:\n",
    "            for chunk in iter(lambda: file.read(8192), b''):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "\n",
    "    md5_value = compute_md5(BASE_FINAL_PATH)\n",
    "    md5_file = BASE_FINAL_PATH.with_suffix('.md5')\n",
    "    md5_file.write_text(md5_value)\n",
    "    print(f\"Checksum MD5 ({md5_value}) guardado en: {md5_file}\")\n",
    "\n",
    "    if BASE_FINAL_PATH.exists():\n",
    "        df_base = pd.read_csv(BASE_FINAL_PATH, parse_dates=['fecha'])\n",
    "        print(\"\\n--- Estructura de Base_Final_Aurelion (desde CSV) ---\")\n",
    "        # Igual ac√°, para ver todas las columnas\n",
    "        df_base.info(verbose=True)\n",
    "\n",
    "        cols_duplicadas_base = df_base.columns[df_base.columns.duplicated()]\n",
    "        print('Columnas duplicadas en Base_Final_Aurelion:', cols_duplicadas_base.tolist())\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(\n",
    "                base_final.reset_index(drop=True),\n",
    "                df_base.reset_index(drop=True),\n",
    "                check_dtype=False,\n",
    "                atol=1e-9,\n",
    "                rtol=1e-9\n",
    "            )\n",
    "            print('El dataframe en memoria coincide con el CSV exportado.')\n",
    "        except AssertionError as err:\n",
    "            print('Diferencias detectadas entre base_final y el CSV exportado.')\n",
    "            print(err)\n",
    "    else:\n",
    "        print('Error: El archivo no se cre√≥ correctamente')\n",
    "\n",
    "    print('Consolidaci√≥n completada exitosamente')\n",
    "else:\n",
    "    print('No se encontr√≥ base_final en memoria. Revise los pasos anteriores.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5bb41a",
   "metadata": {},
   "source": [
    "## üìã 8. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b7d8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      " RESUMEN FINAL - CONSOLIDACION PROYECTO AURELION\n",
      "====================================================================================================\n",
      " Objetivo cumplido:\n",
      "    Se integraron los 4 dataframes limpios (clientes, ventas, detalle_ventas, productos)\n",
      "    Se aplicaron relaciones de clave primaria (PK) y foranea (FK)\n",
      "    Se valido integridad referencial\n",
      "    Se realizo preprocesamiento y limpieza adicional\n",
      "    Se exporto base consolidada a CSV\n",
      " Estadisticas finales:\n",
      "   - Registros en base final: 343\n",
      "   - Columnas integradas: 21\n",
      "   - Memoria usada: 0.14 MB\n",
      "   - Ubicacion archivo CSV: c:\\Users\\Asus\\Desktop\\Augusto Villegas - Proyecto Aurelion\\db\\final\\Base_Final_Aurelion.csv\n",
      " Proximos pasos:\n",
      "   1. Usar Base_Final_Aurelion.csv para analisis avanzados\n",
      "   2. Crear visualizaciones estrategicas\n",
      "   3. Desarrollar modelos predictivos o de segmentacion\n",
      "   4. Generar reportes ejecutivos\n",
      " Consolidacion finalizada correctamente\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('='*100)\n",
    "print(' RESUMEN FINAL - CONSOLIDACION PROYECTO AURELION')\n",
    "print('='*100)\n",
    "\n",
    "print(' Objetivo cumplido:')\n",
    "print('    Se integraron los 4 dataframes limpios (clientes, ventas, detalle_ventas, productos)')\n",
    "print('    Se aplicaron relaciones de clave primaria (PK) y foranea (FK)')\n",
    "print('    Se valido integridad referencial')\n",
    "print('    Se realizo preprocesamiento y limpieza adicional')\n",
    "print('    Se exporto base consolidada a CSV')\n",
    "\n",
    "print(' Estadisticas finales:')\n",
    "if 'base_final' in locals():\n",
    "    print(f\"   - Registros en base final: {len(base_final):,}\")\n",
    "    print(f\"   - Columnas integradas: {len(base_final.columns)}\")\n",
    "    print(f\"   - Memoria usada: {base_final.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    print(f\"   - Ubicacion archivo CSV: {BASE_FINAL_PATH}\")\n",
    "\n",
    "print(' Proximos pasos:')\n",
    "print('   1. Usar Base_Final_Aurelion.csv para analisis avanzados')\n",
    "print('   2. Crear visualizaciones estrategicas')\n",
    "print('   3. Desarrollar modelos predictivos o de segmentacion')\n",
    "print('   4. Generar reportes ejecutivos')\n",
    "\n",
    "print(' Consolidacion finalizada correctamente')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
